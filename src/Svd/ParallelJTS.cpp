#include <atomic>
#include <functional>

#include "BS_thread_pool.hpp"
#include "Svd/ParallelJTS.hpp"
#include "Utils/Logger.hpp"
#include "Utils/UtilityFunctions.hpp"

using namespace GAMM;

void ParallelJTS::startSvd(Matrix matrix) {
  INTELLI_ASSERT(matrix.rows() == matrix.cols(),
                 "Only square matrices supported");

  u = std::move(matrix);
  v = Matrix::Identity(u.rows(), u.cols());
  iterNumber = 0;
  delta = options.tol * u.squaredNorm();

  counter.store(0, std::memory_order_relaxed);

  p1.resize(nColumnPairs());
  p2.resize(nColumnPairs());
  q.resize(npivots());

  auto t = getT();

  std::vector<std::pair<std::mutex, size_t>> temp{t};
  sortLocks.swap(temp);

  pToUse =
      UtilityFunctions::trailingZeros(UtilityFunctions::nextPowerOfTwo(t)) % 2;
}

bool ParallelJTS::svdStep() { return svdStep(1); }
bool ParallelJTS::svdStep(size_t nsteps) {
  auto t = getT();
  BS::multi_future<void> tasks(t - 1);

  for (size_t i = 1; i < t; ++i) {
    tasks[i - 1] = pool->submit([this, nsteps, i]() { workerTask(i, nsteps); });
  }
  auto result = workerTask(0, nsteps);
  tasks.wait();

  return result;
}

bool ParallelJTS::workerTask(size_t workerId, size_t nsteps) {
  // Do not go past maxSweeps specified by the user
  size_t maxIters = std::min(iterNumber + nsteps, options.maxSweeps);

  for (size_t i = iterNumber; i < maxIters; ++i) {
    // ============  PHASE 1  ============
    // Generate columns on which rotations will be applied
    workerTaskPhase1(workerId);

    // Wait for MAIN_WORKER to sort and (possibly) set exit condition
    barrier.arrive_and_wait();

    // Barrier takes care of synchronisation, so relaxed ordering is fine
    if (counter.load(std::memory_order_relaxed) == COMPLETED) {
      return true;
    }
    // ===================================

    // ============  PHASE 2  ============
    // Generate rotations based on the top 1/tau fraction of column-pairs (in
    // parallel).
    workerTaskPhase2(workerId);
    // ===================================

    // Wait for all threads to finish generating the Jacobi rotations.
    barrier.arrive_and_wait();

    // ============  PHASE 3  ============
    // Apply Jacobi rotations
    // Non parallel -- only MAIN_WORKER applies all rotations
    workerTaskPhase3(workerId);
    // ===================================

    // Wait for iteration to complete so next iteration can be started
    barrier.arrive_and_wait();
  }

  if (workerId == 0) {
    iterNumber = maxIters;
  }

  return iterNumber == options.maxSweeps;
}

void ParallelJTS::workerTaskPhase1(size_t workerId) {
  size_t n = u.cols();
  auto t = getT();

  // The start index of the part of p to be written to by this thread, and
  // the number of elements to write
  auto totalNouterIters = n / 2;
  auto nInnerIters = n + (n % 2) - 1;

  auto [outerIterStartI, nOuterIters] =
      UtilityFunctions::unevenDivide(workerId, totalNouterIters, t);

  auto startI = outerIterStartI * nInnerIters;
  auto nelements = nOuterIters * nInnerIters;

  for (auto j1 = workerId, i = startI; j1 < totalNouterIters; j1 += t) {
    size_t j2 = n - j1 - 1 - (n % 2);

    // ------------ PHASE 1.1 ------------
    // Generate all the possible column pairs (in parallel). Then sort each
    // subset of column-pairs generated by each thread (in parallel).
    for (size_t k = j1 + 1, j = j1; k < n; ++k, ++i) {
      auto d = u.col(j).dot(u.col(k));
      std::construct_at(&p1[i], j, k, d);
    }

    for (size_t k = j2 + 1, j = j2; k < n; ++k, ++i) {
      auto d = u.col(j).dot(u.col(k));
      std::construct_at(&p1[i], j, k, d);
    }
  }

  std::sort(p1.begin() + startI, p1.begin() + startI + nelements,
            std::greater<ColumnPair>());

  auto &sortLock = sortLocks[workerId];
  const std::lock_guard<std::mutex> ourGuard{sortLock.first};

  // Wait for all threads to fill p and to have their own locks
  barrier.arrive_and_wait();

  // ------------ PHASE 1.2 ------------
  // Merge sorted subsets of column-pairs (in parallel). The main worker also
  // sets a flag to indicate completion.
  auto nruns = UtilityFunctions::trailingZeros(
      workerId | UtilityFunctions::nextPowerOfTwo(t));

  auto curP = p1.begin() + startI;
  auto mergeToP = p2.begin() + startI;

  for (size_t i = 0; i < nruns; ++i) {
    auto otherId = workerId + (1 << i);
    if (otherId >= t) {
      break;
    }

    auto &otherLock = sortLocks[otherId];
    const std::lock_guard<std::mutex> otherGuard{otherLock.first};

    // Our slice of p
    auto ourStart = curP;
    auto ourEnd = curP + nelements;
    // Other's slice of p
    auto otherStart = curP + nelements;
    auto otherEnd = curP + nelements + otherLock.second;

    std::merge(ourStart, otherStart, ourEnd, otherEnd, mergeToP,
               std::greater<ColumnPair>());

    std::swap(curP, mergeToP);

    nelements += otherLock.second;
  }

  // Update the count of elements we've merged
  sortLock.second = nelements;

  if (workerId == 0) {
    auto hasCompleted = (getP()[0].d < delta) ? COMPLETED : 0;
    counter.store(hasCompleted, std::memory_order_relaxed);
  }
}

void ParallelJTS::workerTaskPhase2(size_t workerId) {
  for (auto i = workerId; i < npivots(); i += getT()) {
    std::construct_at(&q[i], getP()[i], u);
  }
}

void ParallelJTS::workerTaskPhase3(size_t workerId) {
  // Only main worker performs any operations in this phase
  if (workerId != 0) {
    return;
  }

  for (auto rotation : q) {
    rotation.applyTo(u);
    rotation.applyTo(v);
  }
}
